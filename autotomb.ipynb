{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43515726",
   "metadata": {},
   "outputs": [],
   "source": [
    "###AutoTomb##\n",
    "##Takes Digital Giza tomb page URL and returns a set of AI (Meshy) generated 3D models corresponding to contemporaneous (i.e., ancient Egyptian) object references mentioned in early 20th century excavation diaries##\n",
    "###Cook2025 - mncook.net###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98961e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##global config\n",
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import openai\n",
    "import base64\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import numpy as np\n",
    "import umap.umap_ as umap\n",
    "from collections import defaultdict\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# API Keys and Endpoints\n",
    "HARVARD_API_KEY = \"...\"\n",
    "MESHY_API_KEY = \"...\"\n",
    "\n",
    "OPENAI_API_BASE = \"...\"\n",
    "MESHY_API_URL = \"...\"\n",
    "\n",
    "# Target Tomb\n",
    "MAIN_TOMB_URL = \"http://giza.fas.harvard.edu/sites/532/full/\"\n",
    "\n",
    "# GPT and Meshy Models\n",
    "GPT_MODEL_NAME = \"gpt-4.1\"\n",
    "TEMPERATURE = 0.0\n",
    "IMAGE_MODEL = \"dall-e-3\"\n",
    "\n",
    "# User input for base directory\n",
    "BASE_DIR = \"...\"\n",
    "\n",
    "# Directory Setup\n",
    "ARTIFACTS_JSON = os.path.join(BASE_DIR, \"artifacts.json\")\n",
    "IMAGES_DIR = os.path.join(BASE_DIR, \"Images\")\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"Models\")\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Create additional subdirectories under \"Models\"\n",
    "GLTF_MODELS_DIR = os.path.join(MODELS_DIR, \"gltf\")\n",
    "os.makedirs(GLTF_MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Ensure artifacts.json exists\n",
    "if not os.path.exists(ARTIFACTS_JSON):\n",
    "    logger.info(\"No artifacts.json found, creating a new one.\")\n",
    "    with open(ARTIFACTS_JSON, 'w') as json_file:\n",
    "        json.dump({\"artifacts\": []}, json_file, indent=2)\n",
    "\n",
    "logger.info(f\"Directories and artifacts.json setup completed at {BASE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e03f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Scrape diary pages and generate prompts\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration from global config\n",
    "PAGE_RANGE = None\n",
    "\n",
    "# Scraper function\n",
    "def scrape_diary_transcriptions(main_url, page_range=None):\n",
    "    logger.info(\"Initializing Selenium Chrome WebDriver.\")\n",
    "    service = ChromeService()\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    results = []\n",
    "    try:\n",
    "        logger.info(f\"Navigating to main tomb page: {main_url}\")\n",
    "        driver.get(main_url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        tomb_name = driver.find_element(By.CSS_SELECTOR, \"body > div.page-header.header-bg-1 > div > header > h1\").text\n",
    "        logger.info(f\"Extracted tomb name: {tomb_name}\")\n",
    "\n",
    "        links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='/diarypages/'][href*='/full/']\")\n",
    "        diary_links = list({elem.get_attribute(\"href\") for elem in links})\n",
    "        logger.info(f\"Found {len(diary_links)} unique diary page links.\")\n",
    "\n",
    "        start, end = page_range if page_range else (0, len(diary_links))\n",
    "        diary_links = diary_links[start:end]\n",
    "        logger.info(f\"Processing diary page range: {start} to {end}\")\n",
    "\n",
    "        for link in diary_links:\n",
    "            logger.info(f\"Processing diary link: {link}\")\n",
    "            driver.get(link)\n",
    "            time.sleep(2)\n",
    "\n",
    "            excavation_date, transcription_text = \"\", \"\"\n",
    "            try:\n",
    "                div = driver.find_element(By.CSS_SELECTOR, \"div.item__overview.text-alt\")\n",
    "                paragraphs = div.find_elements(By.TAG_NAME, \"p\")\n",
    "                transcription_text = \"\\n\".join(p.text for p in paragraphs)\n",
    "\n",
    "                lines = transcription_text.split(\"\\n\")\n",
    "                date_match = re.search(r\"(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday),.*?\\d{4}\", lines[1])\n",
    "                if date_match:\n",
    "                    excavation_date = date_match.group().strip()\n",
    "                    logger.info(f\"Extracted excavation date: {excavation_date}\")\n",
    "                else:\n",
    "                    logger.warning(f\"No excavation date found in {link}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Issue parsing transcription div or date for {link}: {e}\")\n",
    "\n",
    "            results.append({\"url\": link, \"text\": transcription_text, \"excavation_date\": excavation_date, \"tomb_id\": tomb_name})\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return results\n",
    "\n",
    "# GPT Parsing Function\n",
    "def parse_diary_text_for_objects(diary_text):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": HARVARD_API_KEY}\n",
    "    url = \"https://go.apis.huit.harvard.edu/ais-openai-direct/v1/chat/completions\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": GPT_MODEL_NAME,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"messages\": [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Extract all contemporaneous ancient physical objects mentioned in the excavation diary. \"\n",
    "                \"For each artifact, output detailed metadata in structured JSON suitable for 3D model generation, including visual attributes.\"\n",
    "                \"Include fragmentary, incomplete, and minor objects. Format:\"\n",
    "                \"{artifact_id, mention, prompt, context, tomb_id, excavation_location(area, coordinates), artifact_attributes(type, material, condition, color, inscriptions_present, orientation, period)}\"\n",
    "            )\n",
    "        }, {\"role\": \"user\", \"content\": diary_text}]\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    if response.status_code != 200:\n",
    "        logger.error(f\"GPT API error ({response.status_code}): {response.text}\")\n",
    "        return []\n",
    "\n",
    "    content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    content = re.sub(r\"```(?:json)?\", \"\", content).replace(\"```\", \"\").strip()\n",
    "\n",
    "    try:\n",
    "        artifacts = json.loads(content)\n",
    "        return artifacts if isinstance(artifacts, list) else artifacts.get(\"artifacts\", [])\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"JSON Parsing Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    pages = scrape_diary_transcriptions(MAIN_TOMB_URL, PAGE_RANGE)\n",
    "\n",
    "    artifacts = []\n",
    "    for page in pages:\n",
    "        if not page[\"text\"].strip():\n",
    "            continue\n",
    "\n",
    "        parsed_artifacts = parse_diary_text_for_objects(page[\"text\"])\n",
    "        for artifact in parsed_artifacts:\n",
    "            artifact.update({\"source_url\": page[\"url\"], \"excavation_date\": page[\"excavation_date\"], \"tomb_id\": page[\"tomb_id\"]})\n",
    "        artifacts.extend(parsed_artifacts)\n",
    "\n",
    "    if os.path.exists(ARTIFACTS_JSON):\n",
    "        with open(ARTIFACTS_JSON, \"r\") as f:\n",
    "            existing_data = json.load(f).get(\"artifacts\", [])\n",
    "        artifacts = existing_data + artifacts\n",
    "\n",
    "    with open(ARTIFACTS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"artifacts\": artifacts}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    logger.info(f\"Scraping and parsing completed in {total_time:.2f} seconds. Data saved to {ARTIFACTS_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Generate images from diary prompt list\n",
    "def generate_gpt_image(prompt, artifact_id):\n",
    "    contextual_prefix = (\"Highly detailed, photorealistic rendering in authentic Old Kingdom Egyptian \"\n",
    "                     \"style, appropriate for archaeological reconstruction. \")\n",
    "\n",
    "    # Updated prompt in the image payload\n",
    "    image_payload = {\n",
    "        \"model\": IMAGE_MODEL,\n",
    "        \"prompt\": f\"{contextual_prefix} {prompt}\",\n",
    "        \"size\": \"1024x1024\",\n",
    "        \"quality\": \"standard\",\n",
    "        \"n\": 1\n",
    "    }\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": HARVARD_API_KEY}\n",
    "    image_url = f\"{OPENAI_API_BASE}/images/generations\"\n",
    "\n",
    "    logger.info(f\"Generating image for artifact ID {artifact_id}.\")\n",
    "    response = requests.post(image_url, headers=headers, json=image_payload)\n",
    "    if response.status_code != 200:\n",
    "        logger.error(f\"Image generation error for artifact ID {artifact_id}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "    image_link = response.json()['data'][0]['url']\n",
    "    image_response = requests.get(image_link)\n",
    "\n",
    "    if image_response.status_code == 200:\n",
    "        image_path = os.path.join(IMAGES_DIR, f\"{artifact_id}.png\")\n",
    "        with open(image_path, 'wb') as img_file:\n",
    "            img_file.write(image_response.content)\n",
    "        logger.info(f\"Image saved: {image_path}\")\n",
    "        return image_path\n",
    "    else:\n",
    "        logger.error(f\"Failed to download generated image for artifact ID {artifact_id}.\")\n",
    "        return None\n",
    "\n",
    "# Main Workflow\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    with open(ARTIFACTS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    artifacts = data.get(\"artifacts\", [])\n",
    "\n",
    "    for artifact in artifacts:\n",
    "        artifact_id = artifact[\"artifact_id\"]\n",
    "        prompt = artifact[\"prompt\"]\n",
    "        image_path = generate_gpt_image(prompt, artifact_id)\n",
    "        artifact[\"image_path\"] = image_path if image_path else \"\"\n",
    "\n",
    "    with open(ARTIFACTS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"artifacts\": artifacts}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    logger.info(f\"Completed in {total_time:.2f} seconds. Updated JSON saved to {ARTIFACTS_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae32cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Meshy 3D model generation script (GLB, textures, 10,000 polycount)\n",
    "\n",
    "# Constants\n",
    "MESHY_STATUS_ENDPOINT = \"https://api.meshy.ai/openapi/v1/image-to-3d/{task_id}\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {MESHY_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load artifact JSON\n",
    "with open(ARTIFACTS_JSON, 'r') as f:\n",
    "    data = json.load(f)\n",
    "artifacts = data.get(\"artifacts\", [])\n",
    "\n",
    "# Existing models check for partial execution\n",
    "existing_models = {os.path.splitext(f)[0] for f in os.listdir(MODELS_DIR) if f.endswith(\".glb\")}\n",
    "\n",
    "def encode_image_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        return f\"data:image/png;base64,{encoded}\"\n",
    "\n",
    "def submit_to_meshy(image_base64):\n",
    "    payload = {\n",
    "        \"image_url\": image_base64,\n",
    "        \"enable_pbr\": True,\n",
    "        \"should_remesh\": True,\n",
    "        \"should_texture\": True,\n",
    "        \"ai_model\": \"meshy-4\",\n",
    "        \"target_polycount\": 10000\n",
    "    }\n",
    "    response = requests.post(MESHY_API_URL, headers=HEADERS, json=payload)\n",
    "    if response.status_code in [200, 202]:\n",
    "        task_id = response.json().get(\"result\")\n",
    "        logger.info(f\"Task submitted successfully: {task_id}\")\n",
    "        return task_id\n",
    "    else:\n",
    "        logger.error(f\"Failed to submit task: {response.status_code}, {response.text}\")\n",
    "        return None\n",
    "\n",
    "def wait_for_task_completion(task_id, timeout=600, interval=15):\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        response = requests.get(MESHY_STATUS_ENDPOINT.format(task_id=task_id), headers=HEADERS)\n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Error polling task {task_id}: {response.status_code}, {response.text}\")\n",
    "            return None\n",
    "        \n",
    "        result = response.json()\n",
    "        status = result.get(\"status\")\n",
    "        logger.info(f\"Task {task_id} status: {status}\")\n",
    "\n",
    "        if status == \"SUCCEEDED\":\n",
    "            return result\n",
    "        elif status in [\"FAILED\", \"CANCELED\"]:\n",
    "            logger.error(f\"Task {task_id} {status.lower()}.\")\n",
    "            return None\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        if elapsed > timeout:\n",
    "            logger.error(f\"Task {task_id} timeout after {timeout} seconds.\")\n",
    "            return None\n",
    "        \n",
    "        time.sleep(interval)\n",
    "\n",
    "def download_file(url, output_path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        logger.info(f\"Downloaded file: {output_path}\")\n",
    "    else:\n",
    "        logger.error(f\"Failed to download file: {response.status_code}, {response.text}\")\n",
    "\n",
    "# Main loop\n",
    "logger.info(\"Starting Meshy 3D GLB model generation and download...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for artifact in artifacts:\n",
    "    artifact_id = artifact[\"artifact_id\"]\n",
    "\n",
    "    if artifact_id in existing_models:\n",
    "        logger.info(f\"Model already exists for artifact {artifact_id}, skipping.\")\n",
    "        artifact[\"model_path\"] = os.path.join(MODELS_DIR, f\"{artifact_id}.glb\")\n",
    "        continue\n",
    "\n",
    "    image_path = artifact.get(\"image_path\")\n",
    "    if not image_path or not os.path.exists(image_path):\n",
    "        logger.warning(f\"No valid image for artifact {artifact_id}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    logger.info(f\"Processing artifact {artifact_id}...\")\n",
    "\n",
    "    image_base64 = encode_image_base64(image_path)\n",
    "    task_id = submit_to_meshy(image_base64)\n",
    "    if not task_id:\n",
    "        continue\n",
    "\n",
    "    task_result = wait_for_task_completion(task_id)\n",
    "    if not task_result:\n",
    "        continue\n",
    "\n",
    "    model_urls = task_result.get(\"model_urls\", {})\n",
    "    glb_url = model_urls.get(\"glb\")\n",
    "    if not glb_url:\n",
    "        logger.error(f\"No GLB model URL found for artifact {artifact_id}.\")\n",
    "        continue\n",
    "\n",
    "    glb_file_path = os.path.join(MODELS_DIR, f\"{artifact_id}.glb\")\n",
    "    download_file(glb_url, glb_file_path)\n",
    "\n",
    "    texture_urls = task_result.get(\"texture_urls\", [{}])[0]\n",
    "    local_texture_paths = {}\n",
    "    for tex_type, tex_url in texture_urls.items():\n",
    "        tex_path = os.path.join(MODELS_DIR, f\"{artifact_id}_{tex_type}.png\")\n",
    "        download_file(tex_url, tex_path)\n",
    "        local_texture_paths[tex_type] = tex_path\n",
    "\n",
    "    artifact[\"model_path\"] = glb_file_path\n",
    "    artifact[\"textures\"] = local_texture_paths\n",
    "\n",
    "# Update JSON\n",
    "with open(ARTIFACTS_JSON, 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "logger.info(f\"All models processed. Total execution time: {total_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab822bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Embeddings, reduction, and model placement coordinates\n",
    "# Configuration\n",
    "HARVARD_API_URL = \"...\"\n",
    "MODEL = \"text-embedding-3-large\"\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# Load artifacts\n",
    "with open(ARTIFACTS_JSON, 'r') as f:\n",
    "    data = json.load(f)\n",
    "artifacts = data.get(\"artifacts\", [])\n",
    "\n",
    "# Update model paths (GLB) if they exist\n",
    "existing_models = {f.replace('.glb', ''): os.path.join(MODELS_DIR, f)\n",
    "                   for f in os.listdir(MODELS_DIR) if f.endswith('.glb')}\n",
    "\n",
    "for artifact in artifacts:\n",
    "    artifact_id = artifact['artifact_id']\n",
    "    if artifact_id in existing_models:\n",
    "        artifact['model_path'] = existing_models[artifact_id]\n",
    "\n",
    "# Filter valid artifacts\n",
    "valid_artifacts = [a for a in artifacts if a.get('model_path') and a.get('mention') and a.get('prompt')]\n",
    "if not valid_artifacts:\n",
    "    logging.error(\"No valid artifacts with model paths found.\")\n",
    "    exit()\n",
    "\n",
    "# Embedding function\n",
    "def get_embedding(text):\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": HARVARD_API_KEY}\n",
    "    data = {\"model\": MODEL, \"input\": text}\n",
    "    response = requests.post(HARVARD_API_URL, headers=headers, json=data)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"data\"][0][\"embedding\"]\n",
    "\n",
    "# Generate embeddings\n",
    "logging.info(f\"Generating embeddings for {len(valid_artifacts)} artifacts...\")\n",
    "embeddings = [get_embedding(f\"{a['mention']} {a['prompt']}\") for a in valid_artifacts]\n",
    "\n",
    "# UMAP 3D reduction\n",
    "logging.info(\"Reducing embeddings to 3D space...\")\n",
    "reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, n_components=3, random_state=42)\n",
    "umap_coords = reducer.fit_transform(embeddings)\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "coords_min, coords_max = np.min(umap_coords, axis=0), np.max(umap_coords, axis=0)\n",
    "normalized_coords = (umap_coords - coords_min) / (coords_max - coords_min)\n",
    "\n",
    "# Spherical projection\n",
    "logging.info(\"Converting to spherical layout...\")\n",
    "azimuths = normalized_coords[:, 0] * 2 * np.pi\n",
    "inclinations = normalized_coords[:, 1] * np.pi\n",
    "radii = 5 + 5 * normalized_coords[:, 2]\n",
    "\n",
    "x_coords = radii * np.sin(inclinations) * np.cos(azimuths)\n",
    "y_coords = radii * np.sin(inclinations) * np.sin(azimuths)\n",
    "z_coords = radii * np.cos(inclinations)\n",
    "\n",
    "# Select type specimens and frequency scaling\n",
    "type_frequency = defaultdict(int)\n",
    "type_to_best_artifact = {}\n",
    "\n",
    "for artifact in valid_artifacts:\n",
    "    artifact_type = artifact[\"artifact_attributes\"][\"type\"]\n",
    "    type_frequency[artifact_type] += 1\n",
    "    current = type_to_best_artifact.get(artifact_type, {\"desc_len\": -1})\n",
    "    length = len(artifact['mention']) + len(artifact['prompt'])\n",
    "    if length > current[\"desc_len\"]:\n",
    "        type_to_best_artifact[artifact_type] = {\"artifact\": artifact, \"desc_len\": length}\n",
    "\n",
    "# Assign position and scale\n",
    "max_freq = max(type_frequency.values())\n",
    "\n",
    "for artifact, x, y, z in zip(valid_artifacts, x_coords, y_coords, z_coords):\n",
    "    artifact_type = artifact[\"artifact_attributes\"][\"type\"]\n",
    "    rep = type_to_best_artifact[artifact_type][\"artifact\"]\n",
    "\n",
    "    artifact[\"vr_position\"] = {\"x\": float(x), \"y\": float(y), \"z\": float(z)}\n",
    "    artifact[\"type_specimen\"] = artifact == rep\n",
    "    if artifact[\"type_specimen\"]:\n",
    "        freq = type_frequency[artifact_type]\n",
    "        artifact[\"vr_scale\"] = round(min(1.0 + np.log(freq), 5.0), 2)\n",
    "    else:\n",
    "        artifact[\"vr_scale\"] = 0.0\n",
    "\n",
    "# Add explanatory key\n",
    "data[\"visual_dimensions_key\"] = {\n",
    "    \"x\": \"Azimuthal angle (semantic direction)\",\n",
    "    \"y\": \"Elevation angle (semantic height)\",\n",
    "    \"z\": \"Radial distance from center (semantic prominence)\",\n",
    "    \"vr_scale\": \"Frequency-scaled size of type specimen\",\n",
    "    \"type_specimen\": \"True if this object represents its artifact type\"\n",
    "}\n",
    "\n",
    "# Save updated JSON\n",
    "with open(ARTIFACTS_JSON, 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "logging.info(\"Artifacts JSON updated with spherical layout, representative scaling, and model paths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cook 2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
